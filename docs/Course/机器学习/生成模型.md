# 生成模型

## 潜变量模型(Latent-Variable Model)

在监督学习中，回归和分类都可以理解为学习条件概率分布$p(y|\mathbf{x};\mathbf{w})$

而在无监督学习中，直接学习$x\to x'$的这个分布很难（也即，怎么从一堆无监督学习中得到$x$的分布并能够根据这个分布重建一个相似的$x$）

有一种方法是引入潜变量$z$，我们学习$x\to z$相对简单，而学习$z\to x'$也相对简单。(这个$z$通常会是某些信息熵很高的信息，也就是关键特征)

另一个角度是，在无监督学习中，一个简单的学习$x$的分布的方法是，假设它一定服从某种分布（钦定），然后学习在这个分布下的参数，比如钦定$p(x,w)=\mathcal{N} (x;\mu \sum)$

这样子可以通过优化学习最优的$\mu$和$\sum$，使得对于一组数据分布最优。但显然没有什么意义，这能够表示的函数太少了。

而我们可以学习两个简单的函数的联合分布，他们可以表示的范围就大了很多（对于一部分函数，可以证明其可以拟合任何函数），比如现在有一个简单的分类函数和一个高斯分布，使用他们导出的边缘概率复杂度会高很多：

$p(x) = \sum_z p(x|z)p(z) = \sum_{k=1}^K \pi_z \mathcal{N}(x| \mu{z}, \sigma_z^2)$

一般来说潜变量模型就是学习到怎么完成$x\to z$和$z\to x'$这两个过程。

然后在潜变量模型里，可以借助边缘概率分布重构$x$的值：$p(x) = \int_z p(x,z) dz$

### 高斯潜变量模型

高斯潜变量模型假设了先验概率和似然函数都是服从高斯分布的。也就是$p(z)=\mathcal(z;0,I)$, $p(x|z)=\mathcal(x;Wz+\mu,\sigma^2I)$

![](img/2025-01-08-22-06-33.png)

训练的目标就是最大化对数似然函数，也即$\max_W \sum_{n=1}^N \log p(x_n)$

由于高斯函数的良好性质，可以直接在指数上做加法，也即满足线性性（大概这个意思），本来$p(x_n)$的边缘积分不好算（因为高维情况下$p(x_n,z_n)$不好求，在这个场景下直接套公式$p(x_n)=\mathcal(x_n;\mu,WW^T+\sigma^2I)$

在这种情况下梯度下降也好做。

### 混合高斯模型

先用一个潜变量的视角，就是潜变量$z$是一个one-hot向量。

![](img/2025-01-08-22-15-30.png)

联合分布是每个不同高斯模型的乘积，指数上是onehot向量中的其中一个值，这个潜变量等价于指示这个数据属于哪一个高斯分布。

![](img/2025-01-08-22-17-12.png)

$$
p(x) = \sum_{k=1}^K \pi_k \mathcal N (x;\mu_k,\sigma_k)
$$

其中$\sum_{k=1}^K \pi_k=1$，是第$k$个分布的权重。

在潜变量模型下的条件概率分布$p(x|z)$和先验概率$p(z)$可以表示为

$$
p(x|z=\mathbf{1}_k) = \mathcal(x;\mu_k,\sigma_k) \\
p(z=\mathbf{1}_k) = \pi_k
$$

![](img/2025-01-08-22-22-51.png)

> 在潜变量模型中，各个概率的含义及其在重建任务中的应用如下：
>
>1. **\( p(x) \)**: 这是数据 \( x \) 的边缘概率分布，表示在所有可能的潜变量 \( z \) 下，数据 \( x \) 出现的概率。它反映了数据的总体分布，通常是复杂且难以直接计算的。
>
>2. **\( p(z) \)**: 这是潜变量 \( z \) 的先验分布，通常假设为简单的分布（如高斯分布），以便于模型处理。
>
>3. **\( p(z|x) \)**: 这是在给定观测数据 \( x \) 的情况下，潜变量 \( z \) 的后验分布。在重建任务中，它用于推理或生成新数据。
>
>4. **\( p(x|z) \)**: 这是在给定潜变量 \( z \) 的情况下，数据 \( x \) 的条件分布，即 likelihood。在生成模型（如 VAE）中，它由解码器建模，用于从 \( z \) 生成 \( x \)。
>
>5. **\( p(x,z) \)**: 这是 \( x \) 和 \( z \) 的联合分布，结合了先验和 likelihood，表示 \( x \) 和 \( z \) 之间的关系。
>
>在重建任务中，这些概率的应用如下：
>
>- 在变分自编码器（VAE）中，编码器用于估计 \( p(z|x) \)，解码器用于估计 \( p(x|z) \)。
>- 训练目标是通过最大化证据下界（ELBO）来优化这些分布，使得生成的数据尽可能接近原始数据。
>- ELBO 包含重构损失（衡量生成数据与原始数据的相似度）和 KL 散度（衡量 \( p(z|x) \) 与先验 \( p(z) \) 之间的差异）。
>
>选择潜变量分布 \( p(z) \) 时，通常选择高斯分布因其数学性质便于处理，但在某些特定任务中，其他分布可能更合适。
>
>在评价重建质量时，除了均方误差（MSE），还可以使用结构相似性指数（SSIM）或感知损失（perceptual loss）等指标，以更好地反映人类视觉的相似度。
>
>总之，理解这些概率的含义及其在模型中的作用，对于掌握潜变量模型至关重要。通过实际编程实现简单的 VAE，可以加深对这些概念的理解。
>
> 实际上最难获得的就是能够正确近似$p(z)$的变分后验$p(z|x)$

一个很好的解答：

![](img/2025-01-08-22-32-43.png)

## EM算法



## Deep Generative Model

...

## Representation Learning

好的表征：表征应该包含尽可能多的语义信息，而尽可能少的冗余特征。

![](img/2024-12-23-14-54-19.png)

![](img/2024-12-23-14-54-34.png)

两种典型的模型结构。第一个是auto-encoder，第二个是VAE（Variational Auto-Encoder）。

PCA可以被视为一个线性的auto-encoder

### VAE

**变分自编码器（Variational Autoencoder, VAE）** 是一种生成模型，属于概率图模型与深度学习结合的范畴。VAE 不仅能够进行降维和特征提取，还能够生成数据，同时保留输入数据的概率分布信息。

以下是对 VAE 的详细介绍：

---

#### **VAE 的核心思想**
VAE 是自编码器的一种扩展，它在普通自编码器的基础上引入了概率建模，假设潜在表示 \( z \) 服从某个先验分布（通常是标准正态分布 \( \mathcal{N}(0, I) \)）。通过学习从输入数据 \( x \) 到潜在分布的映射，VAE 能够在潜在空间中生成新数据。

##### **与普通自编码器的区别**
- 普通自编码器学习一个固定的潜在表示 \( z \)，VAE 学习一个分布 \( p(z) \)。
- VAE 的解码器能够基于潜在分布生成新样本，而普通自编码器只能重构输入数据。

---

#### **VAE 的模型结构**

VAE 的结构分为两部分：
1. **编码器（Encoder）**：
   
   - 将输入数据 \( x \) 映射为潜在变量 \( z \) 的分布参数（均值 \( \mu \) 和标准差 \( \sigma \)）：s$q_\phi(z|x) = \mathcal{N}(z|\mu, \sigma^2)$

   - 输出：均值 \( \mu \) 和标准差 \( \sigma \)。

2. **解码器（Decoder）**：
   - 从潜在变量 \( z \) 重构输入数据 \( x \)：$p_\theta(x|z)$
   - 解码器试图最大化重构输入数据 \( x' \) 与真实数据 \( x \) 的相似性。

##### **重参数化技巧（Reparameterization Trick）**

为了使 \( z \) 的采样过程可微（以便优化），VAE 使用重参数化技巧：$z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)$

通过引入标准正态分布噪声 \( \epsilon \)，使采样过程分离为可学习的参数 \( \mu, \sigma \) 和固定的噪声。

---

#### **VAE 的训练目标**

VAE 的目标是最大化输入数据 \( x \) 的边缘对数似然：$\log p_\theta(x) = \int p_\theta(x|z)p(z)dz$

由于直接计算该积分不可行，VAE 通过变分推断引入证据下界（ELBO）作为优化目标：$\mathcal{L}_{\text{ELBO}} = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{\text{KL}}(q_\phi(z|x) \| p(z))$

分为两部分：

-  **重构误差**（第一项）：

      - 使重构样本 \( x' \) 与真实数据 \( x \) 越接近越好。
      - 使用对数似然交叉熵/MSE作为损失函数。

-  **KL 散度**（第二项）：

   - 让近似后验分布 \( q_\phi(z|x) \) 与先验分布 \( p(z) \) 越接近越好。
   - KL散度的表达式：$D_{KL}(q_\phi(z|x)||p(z)) = \int q_\phi(z|x) \log \frac {q_\phi(z|x)} {p(z)} dz$

---

#### **VAE 的生成过程**

1. **训练阶段**：
   - 输入数据 \( x \)，编码器生成 \( z \) 的分布参数 \( \mu \) 和 \( \sigma \)。
   - 解码器基于 \( z \) 重构 \( x' \)，并通过 ELBO 优化模型参数。

2. **生成阶段**：
   - 从先验分布 \( p(z) = \mathcal{N}(0, I) \) 中采样潜在变量 \( z \)。
   - 使用解码器生成数据 \( x' \)，即：
     \[
     x' \sim p_\theta(x|z)
     \]

---
